{
  "posts": [
    {
      "id": "self-host-llm-continue-ollama",
      "title": "Self-Hosting Large Language Models with Continue.dev & Ollama",
      "slug": "self-host-llm-continue-ollama",
      "category": "tutorial",
      "categoryLabel": "Tutorial",
      "date": "2025-11-14",
      "readTime": "12 min read",
      "author": {
        "name": "Duc Le",
        "avatar": "data/images/image_avatar.jpg",
        "title": "AI Engineer & Researcher"
      },
      "excerpt": "A comprehensive guide to self-hosting large language models (up to 32B parameters) using Continue.dev and Ollama on your local server. Learn how to configure network access, set up domain routing, and integrate with your IDE for a fully private AI coding assistant.",
      "image": "https://via.placeholder.com/1200x600",
      "tags": ["Continue.dev", "Ollama", "Self-Hosting", "LLM", "AI Assistant", "Privacy", "Local Development"],
      "featured": true,
      "sections": [
        {
          "id": "introduction",
          "title": "1. Introduction",
          "content": "In the era of AI-assisted development, privacy and control over your data are paramount. While cloud-based AI coding assistants like GitHub Copilot are convenient, they send your code to external servers. Self-hosting your own LLM gives you:\n\n• **Complete Privacy**: Your code never leaves your infrastructure\n• **Cost Control**: No per-user subscription fees\n• **Model Choice**: Run any model up to 32B parameters (or larger with enough resources)\n• **Network Flexibility**: Access from multiple machines via domain routing\n• **No Rate Limits**: Use as much as your hardware allows\n\nThis guide shows you how to combine **Continue.dev** (an open-source AI coding assistant) with **Ollama** (an easy LLM runtime) to create your own private coding assistant."
        },
        {
          "id": "overview",
          "title": "2. Architecture Overview",
          "content": "The setup involves three main components:",
          "nestedList": [
            {
              "text": "Ollama Server (Backend)",
              "children": [
                "Runs on your local server or powerful workstation",
                "Hosts the LLM (e.g., Qwen 2.5 Coder 32B, DeepSeek Coder 33B)",
                "Exposes API on port 11434",
                "Configured to accept external connections (0.0.0.0)"
              ]
            },
            {
              "text": "Network Layer (Access)",
              "children": [
                "Systemd service configured to bind to 0.0.0.0",
                "Firewall rules to allow port 11434",
                "Optional: Domain name pointing to your server",
                "Optional: HTTPS with reverse proxy (nginx/caddy)"
              ]
            },
            {
              "text": "Continue.dev (Client)",
              "children": [
                "VS Code extension installed on development machines",
                "Configured to connect to Ollama server via domain or IP",
                "Provides autocomplete, chat, and refactoring features",
                "Works seamlessly with your local models"
              ]
            }
          ]
        },
        {
          "id": "prerequisites",
          "title": "3. Prerequisites",
          "content": "**Server Requirements:**\n\n• Linux server (Ubuntu 22.04+ recommended)\n• GPU with at least 24GB VRAM (for 32B models)\n  - NVIDIA RTX 4090 (24GB)\n  - NVIDIA A100 (40GB/80GB)\n  - Or multiple GPUs\n• 32GB+ RAM\n• 100GB+ free disk space\n\n**Client Requirements:**\n\n• VS Code installed\n• Network access to your server\n• (Optional) Domain name for easier access\n\n**Software:**\n\n• Ollama (will be installed)\n• systemd (standard on most Linux distributions)\n• (Optional) nginx or Caddy for HTTPS"
        },
        {
          "id": "install-ollama",
          "title": "4. Installing Ollama on Your Server",
          "content": "**Step 1: Install Ollama**\n\nSSH into your server and run:\n\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\nThis installs Ollama and sets up the systemd service.\n\n**Step 2: Verify Installation**\n\n```bash\nollama --version\n# Expected output: ollama version 0.x.x\n```\n\n**Step 3: Pull a Model**\n\nFor coding, I recommend these models:\n\n```bash\n# 32B parameter model (requires 24GB+ VRAM)\nollama pull qwen2.5-coder:32b\n\n# Alternative: 14B model (requires 12GB+ VRAM)\nollama pull qwen2.5-coder:14b\n\n# Alternative: DeepSeek Coder 33B\nollama pull deepseek-coder:33b\n```\n\n**Step 4: Test Locally**\n\n```bash\nollama run qwen2.5-coder:32b\n# Try: \"Write a Python function to reverse a string\"\n```\n\nIf this works, Ollama is ready!"
        },
        {
          "id": "configure-network",
          "title": "5. Configuring Ollama for Network Access",
          "content": "By default, Ollama only listens on `127.0.0.1` (localhost). To allow external access, we need to configure the systemd service.\n\n**Step 1: Create Systemd Override**\n\n```bash\nsudo systemctl edit ollama.service\n```\n\nThis opens an editor. Add:\n\n```ini\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\n```\n\nSave and exit (Ctrl+X, then Y, then Enter in nano).\n\n**Step 2: Reload and Restart**\n\n```bash\nsudo systemctl daemon-reload\nsudo systemctl restart ollama.service\n```\n\n**Step 3: Verify Service Status**\n\n```bash\nsudo systemctl status ollama.service\n```\n\nYou should see:\n- **Active: active (running)**\n- Environment variable showing `OLLAMA_HOST=0.0.0.0:11434`\n\n**Step 4: Check Network Binding**\n\n```bash\nsudo netstat -tulnp | grep 11434\n# or\nsudo ss -tulnp | grep 11434\n```\n\nExpected output:\n```\ntcp   0   0   0.0.0.0:11434   0.0.0.0:*   LISTEN   12345/ollama\n```\n\nIf you see `127.0.0.1:11434`, the configuration didn't apply correctly."
        },
        {
          "id": "firewall-setup",
          "title": "6. Firewall Configuration",
          "content": "**Option A: UFW (Ubuntu Firewall)**\n\n```bash\n# Allow from specific IP (recommended for security)\nsudo ufw allow from 192.168.1.0/24 to any port 11434\n\n# Or allow from anywhere (less secure)\nsudo ufw allow 11434/tcp\n\n# Verify\nsudo ufw status\n```\n\n**Option B: firewalld (RHEL/CentOS)**\n\n```bash\nsudo firewall-cmd --permanent --add-port=11434/tcp\nsudo firewall-cmd --reload\n```\n\n**Option C: iptables**\n\n```bash\nsudo iptables -A INPUT -p tcp --dport 11434 -j ACCEPT\nsudo iptables-save | sudo tee /etc/iptables/rules.v4\n```\n\n**Security Note:**\n\nFor production use, restrict access to trusted IPs only:\n\n```bash\nsudo ufw allow from YOUR_CLIENT_IP to any port 11434\n```"
        },
        {
          "id": "domain-setup",
          "title": "7. Setting Up Domain Access (Optional but Recommended)",
          "content": "Instead of using IP addresses, set up a domain for easier access.\n\n**Step 1: Configure DNS**\n\nAdd an A record in your DNS provider:\n\n```\nType: A\nName: ollama (or ai, llm, etc.)\nValue: YOUR_SERVER_IP\nTTL: 300\n```\n\nResult: `ollama.yourdomain.com` → `YOUR_SERVER_IP`\n\n**Step 2: Test DNS Resolution**\n\n```bash\nnslookup ollama.yourdomain.com\n# Should return your server IP\n```\n\n**Step 3: Set Up HTTPS with Caddy (Recommended)**\n\nInstall Caddy:\n\n```bash\nsudo apt install -y debian-keyring debian-archive-keyring apt-transport-https\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | sudo tee /etc/apt/sources.list.d/caddy-stable.list\nsudo apt update\nsudo apt install caddy\n```\n\nCreate Caddyfile:\n\n```bash\nsudo nano /etc/caddy/Caddyfile\n```\n\nAdd:\n\n```\nollama.yourdomain.com {\n    reverse_proxy localhost:11434\n}\n```\n\nReload Caddy:\n\n```bash\nsudo systemctl reload caddy\n```\n\nCaddy automatically obtains and renews SSL certificates from Let's Encrypt!\n\n**Step 4: Test HTTPS Access**\n\n```bash\ncurl https://ollama.yourdomain.com/api/tags\n```\n\nYou should see a JSON response with available models."
        },
        {
          "id": "configure-continue",
          "title": "8. Configuring Continue.dev",
          "content": "**Step 1: Install Continue.dev Extension**\n\n1. Open VS Code\n2. Go to Extensions (Ctrl+Shift+X)\n3. Search for \"Continue\"\n4. Click Install\n\n**Step 2: Configure Ollama Connection**\n\n1. Click the Continue icon in the sidebar\n2. Click the gear icon (⚙️) to open config\n3. This opens `~/.continue/config.json`\n\n**Step 3: Edit Configuration**\n\nReplace the content with:\n\n```json\n{\n  \"models\": [\n    {\n      \"title\": \"Qwen 2.5 Coder 32B\",\n      \"provider\": \"ollama\",\n      \"model\": \"qwen2.5-coder:32b\",\n      \"apiBase\": \"https://ollama.yourdomain.com\"\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"Qwen 2.5 Coder 32B\",\n    \"provider\": \"ollama\",\n    \"model\": \"qwen2.5-coder:32b\",\n    \"apiBase\": \"https://ollama.yourdomain.com\"\n  },\n  \"allowAnonymousTelemetry\": false,\n  \"embeddingsProvider\": {\n    \"provider\": \"ollama\",\n    \"model\": \"nomic-embed-text\",\n    \"apiBase\": \"https://ollama.yourdomain.com\"\n  }\n}\n```\n\n**If not using HTTPS/domain, use IP directly:**\n\n```json\n\"apiBase\": \"http://YOUR_SERVER_IP:11434\"\n```\n\n**Step 4: Reload VS Code**\n\nPress `Ctrl+Shift+P` → \"Developer: Reload Window\"\n\n**Step 5: Test Connection**\n\n1. Click Continue sidebar icon\n2. Type a question: \"Explain what a REST API is\"\n3. You should get a response from your self-hosted model!"
        },
        {
          "id": "usage-features",
          "title": "9. Using Continue.dev Features",
          "content": "**Chat Interface**\n\nClick the Continue icon → Chat tab\n- Ask coding questions\n- Get explanations of code\n- Debug issues\n\n**Autocomplete**\n\nAs you type code, Continue suggests completions:\n- Press `Tab` to accept\n- Press `Esc` to dismiss\n\n**Inline Editing**\n\n1. Select code\n2. Press `Ctrl+I` (or `Cmd+I` on Mac)\n3. Type instruction: \"Add error handling\"\n4. Continue modifies the code directly\n\n**Refactoring**\n\nRight-click code → Continue → Choose action:\n- \"Explain this code\"\n- \"Write unit tests\"\n- \"Optimize this code\"\n- \"Add comments\"\n\n**Codebase Context**\n\nContinue can reference your entire codebase:\n\n```\n@codebase How does authentication work in this project?\n```"
        },
        {
          "id": "performance-optimization",
          "title": "10. Performance Optimization",
          "content": "**GPU Memory Management**\n\nIf running multiple models, configure Ollama to limit memory:\n\n```bash\nsudo systemctl edit ollama.service\n```\n\nAdd:\n\n```ini\n[Service]\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=2\"\nEnvironment=\"OLLAMA_NUM_PARALLEL=2\"\nEnvironment=\"OLLAMA_MAX_VRAM=20GB\"\n```\n\n**Model Quantization**\n\nUse quantized models for better performance:\n\n```bash\n# Q4 quantization (lower quality, faster)\nollama pull qwen2.5-coder:32b-q4\n\n# Q5 quantization (balanced)\nollama pull qwen2.5-coder:32b-q5\n\n# Q8 quantization (higher quality)\nollama pull qwen2.5-coder:32b-q8\n```\n\n**Context Window**\n\nAdjust context size in Continue config:\n\n```json\n{\n  \"models\": [{\n    \"contextLength\": 8192,\n    \"completionOptions\": {\n      \"temperature\": 0.2,\n      \"topP\": 0.9,\n      \"maxTokens\": 2048\n    }\n  }]\n}\n```"
        },
        {
          "id": "troubleshooting",
          "title": "11. Troubleshooting Common Issues",
          "content": "**Issue: Connection Refused**\n\nCheck if Ollama is listening on 0.0.0.0:\n\n```bash\nsudo netstat -tulnp | grep 11434\n```\n\nIf showing 127.0.0.1, verify systemd override:\n\n```bash\nsudo systemctl cat ollama.service | grep OLLAMA_HOST\n```\n\n**Issue: Slow Responses**\n\n1. Check GPU utilization:\n\n```bash\nnvidia-smi\n```\n\n2. Use smaller/quantized model\n3. Reduce context length\n4. Increase `num_gpu` layers:\n\n```bash\nollama run qwen2.5-coder:32b --num-gpu 40\n```\n\n**Issue: Model Not Found**\n\nList available models:\n\n```bash\nollama list\n```\n\nPull if missing:\n\n```bash\nollama pull qwen2.5-coder:32b\n```\n\n**Issue: Certificate Errors (HTTPS)**\n\nIf using self-signed certificates, disable SSL verification:\n\n```json\n{\n  \"models\": [{\n    \"apiBase\": \"https://ollama.yourdomain.com\",\n    \"requestOptions\": {\n      \"verifySsl\": false\n    }\n  }]\n}\n```\n\n**Issue: High Memory Usage**\n\nLimit loaded models:\n\n```bash\nsudo systemctl edit ollama.service\n```\n\n```ini\n[Service]\nEnvironment=\"OLLAMA_MAX_LOADED_MODELS=1\"\n```"
        },
        {
          "id": "security-best-practices",
          "title": "12. Security Best Practices",
          "content": "**Network Security**\n\n• Use HTTPS (Caddy/nginx with SSL)\n• Restrict firewall to trusted IPs only\n• Use VPN for remote access instead of exposing to internet\n• Implement API authentication (see Caddy basic auth)\n\n**Example: Caddy with Basic Auth**\n\n```\nollama.yourdomain.com {\n    reverse_proxy localhost:11434\n    basicauth {\n        user $2a$14$Zkx19XLiW6VYouLHR5NmfOFU0z2GTNmpkT/5qqR7hx7wHfajoKqO\n    }\n}\n```\n\nGenerate password hash:\n\n```bash\ncaddy hash-password\n```\n\n**Data Privacy**\n\n• Models run entirely on your infrastructure\n• No data sent to third parties\n• Logs stored locally (check `/var/log/ollama/`)\n• Consider encrypting model files at rest\n\n**Access Control**\n\n• Use SSH key authentication only\n• Disable password authentication\n• Keep Ollama and system updated\n• Monitor access logs regularly"
        },
        {
          "id": "model-recommendations",
          "title": "13. Recommended Models for Coding",
          "content": "**For 24GB VRAM (RTX 4090, A5000):**\n\n```bash\n# Best overall coding model\nollama pull qwen2.5-coder:32b\n\n# Alternative: DeepSeek Coder\nollama pull deepseek-coder:33b\n\n# For faster inference\nollama pull qwen2.5-coder:32b-q4\n```\n\n**For 12-16GB VRAM (RTX 3090, RTX 4080):**\n\n```bash\nollama pull qwen2.5-coder:14b\nollama pull codellama:13b\nollama pull deepseek-coder:6.7b\n```\n\n**For 8GB VRAM (RTX 3070, RTX 4060 Ti):**\n\n```bash\nollama pull qwen2.5-coder:7b\nollama pull codellama:7b\nollama pull starcoder2:7b\n```\n\n**Embedding Models (for codebase search):**\n\n```bash\nollama pull nomic-embed-text\nollama pull mxbai-embed-large\n```"
        },
        {
          "id": "comparison",
          "title": "14. Comparison: Self-Hosted vs Cloud",
          "content": "**Self-Hosted (Ollama + Continue.dev)**\n\n✅ **Pros:**\n• Complete privacy and data control\n• No subscription costs after hardware investment\n• Unlimited usage\n• Customizable models\n• Works offline\n• Lower latency (local network)\n\n❌ **Cons:**\n• High upfront hardware cost ($2000+ for GPU)\n• Maintenance required\n• Power consumption\n• Limited to your hardware capacity\n\n**Cloud-Based (GitHub Copilot, Cursor)**\n\n✅ **Pros:**\n• No hardware required\n• Always up-to-date models\n• No maintenance\n• Works from anywhere\n\n❌ **Cons:**\n• $10-20/month per user\n• Code sent to external servers\n• Rate limits\n• Requires internet\n• Privacy concerns for proprietary code",
          "table": {
            "headers": ["Feature", "Self-Hosted", "GitHub Copilot"],
            "rows": [
              ["Privacy", "100% private", "Code sent to GitHub"],
              ["Cost (yearly)", "$500 (power)", "$120-240/user"],
              ["Model Size", "Up to 70B+", "Unknown (proprietary)"],
              ["Latency", "10-100ms (local)", "200-500ms"],
              ["Offline Mode", "✅ Yes", "❌ No"],
              ["Customization", "✅ Full control", "❌ Limited"],
              ["Maintenance", "❌ Required", "✅ None"]
            ]
          }
        },
        {
          "id": "real-world-usage",
          "title": "15. Real-World Usage Example",
          "content": "Here's how I use this setup in my daily workflow:\n\n**Morning: Code Review**\n\n1. Open PR in VS Code\n2. Select code → Continue → \"Review this for security issues\"\n3. Model suggests improvements without sending code externally\n\n**Afternoon: Feature Development**\n\n1. Type function signature\n2. Continue autocompletes implementation\n3. Press Tab to accept, modify as needed\n4. Ask in chat: \"Write unit tests for this function\"\n5. Continue generates test cases\n\n**Evening: Debugging**\n\n1. Encounter error in logs\n2. Copy stack trace to Continue chat\n3. Ask: \"@codebase Why is this error occurring?\"\n4. Model searches codebase and explains root cause\n\n**Hardware Setup:**\n\n• Server: Ubuntu 22.04 with NVIDIA RTX 4090 (24GB)\n• Model: Qwen 2.5 Coder 32B Q5 quantization\n• Network: Local domain via Caddy (ollama.homelab.local)\n• Access: 3 developer machines + 1 laptop\n\n**Performance:**\n\n• Autocomplete latency: ~50ms\n• Chat response: ~2 seconds for 200 tokens\n• Context window: 32K tokens\n• GPU utilization: 60-80% during inference"
        },
        {
          "id": "conclusion",
          "title": "16. Conclusion",
          "content": "Self-hosting LLMs with Continue.dev and Ollama provides a powerful, private alternative to cloud-based coding assistants. While it requires initial investment in hardware, the benefits are substantial:\n\n**Key Takeaways:**\n\n• Configure Ollama systemd service to bind to 0.0.0.0 for network access\n• Use domain names and HTTPS for production setups\n• 24GB VRAM allows running 32B parameter models effectively\n• Continue.dev integrates seamlessly with self-hosted Ollama\n• Total privacy: your code never leaves your infrastructure\n\n**Next Steps:**\n\n1. Start with a smaller model to test your setup\n2. Monitor GPU usage and optimize quantization levels\n3. Experiment with different models for your use case\n4. Set up monitoring (Prometheus + Grafana) for production use\n5. Consider adding more GPUs for larger models or multiple users\n\n**Resources:**\n\n• [Ollama Documentation](https://ollama.com/docs)\n• [Continue.dev Documentation](https://continue.dev/docs)\n• [Model Leaderboard](https://ollama.com/library)\n\nWith this setup, you have complete control over your AI coding assistant while maintaining the convenience of modern AI-powered development tools. Whether you're working on proprietary code, in a regulated industry, or simply value privacy, self-hosting is a viable and powerful option."
        }
      ]
    },
    {
      "id": "mcp-streaming-limitation",
      "title": "MCP Tool Calling: Why Streaming Isn't Supported",
      "slug": "mcp-streaming-limitation",
      "category": "tutorial",
      "categoryLabel": "Tutorial",
      "date": "2025-11-13",
      "readTime": "8 min read",
      "author": {
        "name": "Duc Le",
        "avatar": "data/images/image_avatar.jpg",
        "title": "AI Engineer & Researcher"
      },
      "excerpt": "Exploring the architectural constraints that prevent Model Context Protocol (MCP) tools from supporting streaming responses, and how this impacts real-time AI applications like chatbots and form assistants.",
      "image": "https://via.placeholder.com/1200x600",
      "tags": ["MCP", "Streaming", "Tool Calling", "LLM", "AI Agent", "Real-time"],
      "featured": true,
      "sections": [
        {
          "id": "introduction",
          "title": "1. Introduction",
          "content": "When building AI agents with Model Context Protocol (MCP), one limitation quickly becomes apparent: MCP tools cannot stream their responses. This constraint affects real-time user experience in applications like chatbots and form assistants. Understanding why this limitation exists helps developers design better architectures and set appropriate expectations."
        },
        {
          "id": "what-is-mcp",
          "title": "2. What is Model Context Protocol (MCP)?",
          "content": "Model Context Protocol is a standardized way for LLMs to interact with external tools and data sources. It provides:\n\n• **Tool Discovery**: LLMs can discover available tools and their capabilities\n• **Structured Calling**: Tools are invoked with well-defined schemas\n• **Context Integration**: Tool results are seamlessly integrated into LLM context\n• **Extensibility**: Easy to add new tools without modifying core logic\n\nMCP is used in applications like:\n- AI agents that need to access databases\n- Form assistants that populate fields automatically\n- Chatbots with external knowledge bases\n- Code assistants with file system access"
        },
        {
          "id": "streaming-in-llms",
          "title": "3. Streaming in LLM Applications",
          "content": "Streaming allows LLMs to send responses token-by-token as they're generated, rather than waiting for the complete response. This provides:\n\n**Benefits:**\n• Perceived faster response time\n• Better user experience with real-time feedback\n• Lower time-to-first-token (TTFT)\n• Progressive disclosure of information\n\n**Common Implementation:**\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\nThis works great for pure text generation, but breaks when tools are involved."
        },
        {
          "id": "why-no-streaming",
          "title": "4. Why MCP Tools Don't Support Streaming",
          "content": "The fundamental incompatibility stems from how tool calling works:",
          "nestedList": [
            {
              "text": "Tool Call Must Be Complete",
              "children": [
                "The LLM must generate the entire tool name and all parameters before the tool can execute",
                "You can't call a function with partial arguments",
                "Example: `get_weather(city=\"San\")` is invalid until it becomes `get_weather(city=\"San Francisco\")`"
              ]
            },
            {
              "text": "Sequential Dependency",
              "children": [
                "Tool result must be received before LLM can continue generating",
                "Cannot stream text that depends on unknown tool results",
                "The LLM needs the tool output to contextualize its next response"
              ]
            },
            {
              "text": "Schema Validation",
              "children": [
                "Tool parameters must validate against the schema",
                "Partial JSON is not valid JSON",
                "Cannot validate `{\"city\": \"San` mid-stream"
              ]
            },
            {
              "text": "Protocol Design",
              "children": [
                "MCP uses request-response pattern, not streaming protocol",
                "Tool servers expect complete requests",
                "No mechanism for partial tool invocations"
              ]
            }
          ]
        },
        {
          "id": "impact-on-ai-agents",
          "title": "5. Impact on AI Agent Applications",
          "content": "This limitation significantly affects user experience in real-time applications:\n\n**Scenario: AI Form Assistant**\n\nConsider an AI agent that fills form fields using MCP tools:\n\n```python\ntools = [\n    {\n        \"name\": \"fill_form_field\",\n        \"description\": \"Fill a form field with extracted data\",\n        \"parameters\": {\n            \"field_name\": {\"type\": \"string\"},\n            \"value\": {\"type\": \"string\"}\n        }\n    }\n]\n```\n\n**Without Streaming:**\n1. User submits query\n2. LLM processes (user sees loading spinner)\n3. LLM decides to call tool\n4. Tool executes\n5. LLM generates final response\n6. User sees complete result (3-5 seconds later)\n\n**With Streaming (Not Possible with Tools):**\n1. User submits query\n2. LLM streams: \"I'll help you fill...\"\n3. Interrupt for tool call\n4. Resume streaming: \"...the form with your information\"\n\nThe interruption breaks the streaming flow, so providers disable streaming entirely when tools are used."
        },
        {
          "id": "evidence-from-providers",
          "title": "6. Evidence from LLM Providers",
          "content": "All major LLM providers have this limitation:",
          "table": {
            "headers": ["Provider", "Streaming with Tools", "Documentation"],
            "rows": [
              ["OpenAI", "Not Supported", "\"When using tools, streaming is not available. The response will be returned once the tool call is complete.\""],
              ["Anthropic Claude", "Partial Support", "Claude can stream text before/after tool calls, but not during tool execution. Tool use events are sent as complete blocks."],
              ["Google Gemini", "Not Supported", "Function calling requires waiting for complete response before tools execute."],
              ["Azure OpenAI", "Not Supported", "Same limitation as OpenAI - tools disable streaming."]
            ]
          }
        },
        {
          "id": "workarounds",
          "title": "7. Workarounds and Best Practices",
          "content": "While you can't stream during tool calls, you can improve UX:\n\n**1. Hybrid Approach - Stream When Possible**\n\n```python\n# Stream initial response\nresponse = await client.chat(\n    messages=messages,\n    stream=True\n)\n\n# If tool call needed, switch to non-streaming\nif requires_tool_call(response):\n    final_response = await client.chat(\n        messages=messages,\n        tools=tools,\n        stream=False  # Can't stream with tools\n    )\n```\n\n**2. Optimistic UI Updates**\n\nUpdate UI immediately while waiting for tool execution:\n\n```javascript\n// Show optimistic update\nshowLoadingIndicator(\"Filling email field...\");\n\n// Call MCP tool (non-streaming)\nconst result = await callMCPTool(\"fill_form_field\", {\n  field_name: \"email\",\n  value: extractedEmail\n});\n\n// Update with actual result\nupdateFormField(\"email\", result.value);\n```\n\n**3. Progress Indicators**\n\nShow detailed progress for multi-step tool operations:\n\n```\n✓ Extracting user information...\n✓ Validating email format...\n⏳ Filling form fields...\n⏳ Verifying data...\n```\n\n**4. Parallel Tool Calls**\n\nSome providers support parallel tool execution:\n\n```python\n# Request multiple tools at once\ntools_to_call = [\n    {\"name\": \"fill_email\", \"args\": {...}},\n    {\"name\": \"fill_name\", \"args\": {...}},\n    {\"name\": \"fill_phone\", \"args\": {...}}\n]\n\n# Execute in parallel (provider-dependent)\nresults = await execute_tools_parallel(tools_to_call)\n```\n\n**5. Client-Side Streaming Simulation**\n\nSimulate streaming for non-tool responses:\n\n```javascript\nfunction simulateStreaming(text, onChunk) {\n  let index = 0;\n  const interval = setInterval(() => {\n    if (index < text.length) {\n      onChunk(text[index]);\n      index++;\n    } else {\n      clearInterval(interval);\n    }\n  }, 20); // 20ms per character\n}\n```"
        },
        {
          "id": "real-world-example",
          "title": "8. Real-World Example: AI Form Assistant",
          "content": "In my AI Agent Form Assistant project, I encountered this limitation when implementing real-time form filling:\n\n**Problem:**\nUsers expected instant feedback as the AI populated form fields, but MCP tool calls required waiting for complete responses.\n\n**Solution Architecture:**\n\n```python\n# Step 1: Stream initial acknowledgment (no tools)\nresponse = await llm.chat(\n    messages=[{\"role\": \"user\", \"content\": user_input}],\n    stream=True\n)\n\nfor chunk in response:\n    yield chunk  # \"I'll help fill your form...\"\n\n# Step 2: Extract data with tool (non-streaming)\ndata = await llm.chat(\n    messages=messages,\n    tools=mcp_tools,\n    stream=False  # Must disable for tools\n)\n\n# Step 3: Fill fields with optimistic UI\nfor field in data.tool_calls:\n    update_ui_optimistically(field.name, field.value)\n    actual_result = await mcp_tool.execute(field)\n    confirm_ui_update(field.name, actual_result)\n\n# Step 4: Stream confirmation (no tools)\nfinal_response = await llm.chat(\n    messages=updated_messages,\n    stream=True\n)\n\nfor chunk in final_response:\n    yield chunk  # \"Form filled successfully!\"\n```\n\n**Key Takeaways:**\n• Split the interaction into streaming and non-streaming phases\n• Use streaming for conversational parts\n• Accept non-streaming for tool execution\n• Provide visual feedback during tool calls\n• Stream confirmation after tools complete"
        },
        {
          "id": "future-possibilities",
          "title": "9. Future Possibilities",
          "content": "**Potential Solutions (Not Yet Available):**\n\n**1. Streaming Tool Parameters**\n\nSome research suggests streaming tool parameters incrementally:\n\n```python\n# Hypothetical future API\nfor chunk in stream:\n    if chunk.tool_call_delta:\n        # Receive partial tool parameters\n        partial_params = chunk.tool_call_delta.parameters\n        update_ui_with_partial_data(partial_params)\n```\n\n**2. Tool Execution Streaming**\n\nTools themselves could stream results:\n\n```python\nfor result_chunk in mcp_tool.execute_streaming(params):\n    yield result_chunk  # Stream tool execution progress\n```\n\n**3. Speculative Tool Calling**\n\nLLM predicts likely tool calls and executes speculatively:\n\n```python\n# LLM streams text while speculatively calling tools\n# If prediction wrong, discard tool results\n# If correct, use cached results immediately\n```\n\n**Current Status:**\nNone of these are available in production. MCP and major LLM providers have not announced plans to support streaming with tools."
        },
        {
          "id": "conclusion",
          "title": "10. Conclusion",
          "content": "MCP tool calling's inability to support streaming is a fundamental architectural limitation, not a temporary bug or oversight. It stems from:\n\n• Tool calls requiring complete parameters before execution\n• Sequential dependency between tool results and LLM generation\n• Schema validation requirements for structured data\n• Request-response protocol design in MCP\n\n**Best Practices:**\n\n• Accept that tools = no streaming\n• Use hybrid approach: stream when possible, don't stream for tools\n• Provide excellent non-streaming UX with progress indicators\n• Implement optimistic UI updates\n• Be transparent with users about processing time\n\n**Looking Forward:**\n\nWhile streaming with tools would improve UX, the architectural challenges are significant. For now, focus on making non-streaming experiences as smooth as possible through clever UI design and clear communication.\n\nIn my AI Agent Form Assistant project, embracing this constraint led to a cleaner architecture with clear phases: conversation (streaming) → tool execution (non-streaming) → confirmation (streaming). Sometimes, constraints breed better design."
        }
      ]
    },
    {
      "id": "n8n-clickup-integration",
      "title": "n8n ClickUp Integration: Access Tokens & Webhook Management",
      "slug": "n8n-clickup-integration",
      "category": "tutorial",
      "categoryLabel": "Tutorial",
      "date": "2025-11-13",
      "readTime": "10 min read",
      "author": {
        "name": "Duc Le",
        "avatar": "data/images/image_avatar.jpg",
        "title": "AI Engineer & Researcher"
      },
      "excerpt": "A comprehensive guide to integrating n8n with ClickUp, covering OAuth access token retrieval, webhook configuration, and solving the common webhook ID conflict error that occurs when multiple workflows share webhooks.",
      "image": "https://via.placeholder.com/1200x600",
      "tags": ["n8n", "ClickUp", "Automation", "Webhooks", "OAuth", "API Integration"],
      "featured": true,
      "sections": [
        {
          "id": "introduction",
          "title": "1. Introduction",
          "content": "n8n is a powerful workflow automation tool that can integrate with ClickUp to automate project management tasks. However, setting up the integration properly requires understanding OAuth authentication and webhook management. This guide covers two critical aspects: obtaining ClickUp access tokens and handling webhook ID conflicts."
        },
        {
          "id": "getting-access-token",
          "title": "2. Getting ClickUp Access Token",
          "content": "ClickUp uses OAuth 2.0 for authentication. Here's the complete process to obtain an access token."
        },
        {
          "id": "create-oauth-app",
          "title": "2.1. Create OAuth App in ClickUp",
          "content": "First, you need to create an OAuth application in ClickUp:\n\n1. Log in to ClickUp\n2. Go to Settings (bottom left corner)\n3. Navigate to **Apps** section\n4. Click **Create an App**\n5. Fill in the application details:\n   - App Name: Your app name (e.g., \"n8n Integration\")\n   - Redirect URL: Your n8n callback URL\n   - For self-hosted n8n: `https://your-n8n-domain.com/rest/oauth2-credential/callback`\n   - For n8n cloud: `https://app.n8n.cloud/rest/oauth2-credential/callback`\n\n6. Click **Create App**\n7. Save the **Client ID** and **Client Secret** - you'll need these!"
        },
        {
          "id": "configure-n8n-credentials",
          "title": "2.2. Configure n8n Credentials",
          "content": "In n8n, set up ClickUp OAuth2 credentials:\n\n1. Go to **Credentials** in n8n\n2. Click **Add Credential**\n3. Search for **ClickUp OAuth2 API**\n4. Enter the credentials:\n   - Client ID: From ClickUp OAuth app\n   - Client Secret: From ClickUp OAuth app\n5. Click **Connect my account**\n6. You'll be redirected to ClickUp to authorize\n7. Select the workspace to grant access\n8. After authorization, you'll be redirected back to n8n\n\nThe access token is now stored securely in n8n!"
        },
        {
          "id": "manual-token-retrieval",
          "title": "2.3. Manual Access Token Retrieval (Alternative)",
          "content": "If you need to get the access token manually (for API testing):\n\n```bash\n# Step 1: Get authorization code\n# Visit this URL in browser (replace CLIENT_ID and REDIRECT_URI):\nhttps://app.clickup.com/api?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI\n\n# Step 2: After authorization, you'll be redirected with a code:\n# https://your-redirect-uri?code=AUTHORIZATION_CODE\n\n# Step 3: Exchange code for access token\ncurl -X POST https://api.clickup.com/api/v2/oauth/token \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"client_id\": \"YOUR_CLIENT_ID\",\n    \"client_secret\": \"YOUR_CLIENT_SECRET\",\n    \"code\": \"AUTHORIZATION_CODE\"\n  }'\n\n# Response:\n# {\n#   \"access_token\": \"YOUR_ACCESS_TOKEN\",\n#   \"token_type\": \"Bearer\"\n# }\n```"
        },
        {
          "id": "webhook-id-problem",
          "title": "3. The Webhook ID Conflict Problem",
          "content": "One of the most common issues when using ClickUp webhooks in n8n is the webhook ID conflict error. This happens when you try to use the same webhook trigger in multiple workflows."
        },
        {
          "id": "error-description",
          "title": "3.1. Error Description",
          "content": "When you duplicate a workflow or try to reuse webhook configurations, you'll encounter:\n\n```\nError: Webhook ID already exists\nClickUp API Error: This webhook endpoint is already registered\n```\n\nThis occurs because:\n- Each webhook in ClickUp must have a unique ID\n- n8n workflows that share webhook nodes may try to register the same webhook multiple times\n- ClickUp API rejects duplicate webhook registrations"
        },
        {
          "id": "why-it-happens",
          "title": "3.2. Why This Happens",
          "content": "Understanding the root cause:",
          "nestedList": [
            {
              "text": "Webhook Registration",
              "children": [
                "When you activate a workflow with ClickUp webhook, n8n registers the webhook with ClickUp API",
                "ClickUp creates a webhook ID and associates it with your endpoint",
                "This webhook ID is stored in n8n's workflow configuration"
              ]
            },
            {
              "text": "Workflow Duplication",
              "children": [
                "When you duplicate a workflow, the webhook node configuration (including webhook ID) is copied",
                "Both workflows now have the same webhook ID",
                "When activating the second workflow, n8n tries to register a webhook that already exists"
              ]
            },
            {
              "text": "ClickUp's Constraint",
              "children": [
                "ClickUp enforces uniqueness: one webhook endpoint per registration",
                "You cannot have multiple workflows listening to the same webhook ID",
                "Each workflow MUST have its own unique webhook"
              ]
            }
          ]
        },
        {
          "id": "solution",
          "title": "4. Solution: Unique Webhooks Per Workflow",
          "content": "Here's the step-by-step solution to fix webhook ID conflicts:"
        },
        {
          "id": "solution-steps",
          "title": "4.1. Implementation Steps",
          "content": "**Step 1: Deactivate All Affected Workflows**\n\nBefore making changes, deactivate all workflows that share the webhook:\n- This prevents registration conflicts\n- Allows ClickUp to clean up existing webhooks\n\n**Step 2: Delete and Recreate Webhook Nodes**\n\nFor each workflow after the first:\n1. Open the workflow in n8n\n2. Delete the ClickUp Webhook Trigger node\n3. Add a new ClickUp Webhook Trigger node\n4. Configure it with the same settings (events, filters, etc.)\n5. Important: Do NOT copy-paste webhook nodes between workflows!\n\n**Step 3: Configure Unique Webhook Settings**\n\nFor each new webhook node:\n- Select the same ClickUp workspace\n- Choose the events you want to listen to\n- Configure filters if needed\n- n8n will generate a new unique webhook ID automatically\n\n**Step 4: Activate Workflows One by One**\n\n1. Activate the first workflow\n2. Wait for successful webhook registration (check n8n logs)\n3. Activate the second workflow\n4. Repeat for remaining workflows\n\n**Step 5: Verify Webhook Registration**\n\nCheck in ClickUp:\n1. Go to Settings → Integrations → Webhooks\n2. You should see separate webhook entries for each workflow\n3. Each webhook has a unique ID and endpoint"
        },
        {
          "id": "best-practices",
          "title": "5. Best Practices",
          "content": "**Webhook Management**\n\n• Never duplicate workflows with webhook triggers - create new ones from scratch\n• Use descriptive webhook names to identify which workflow they belong to\n• Document your webhook configurations\n• Regularly audit and clean up unused webhooks in ClickUp\n\n**Access Token Security**\n\n• Store tokens securely in n8n credentials, never hardcode\n• Use environment variables for sensitive data\n• Implement token refresh logic for long-running workflows\n• Rotate tokens periodically for security\n\n**Error Handling**\n\n• Implement retry logic with exponential backoff for API calls\n• Monitor webhook delivery failures\n• Set up alerting for workflow errors\n• Log all API interactions for debugging\n\n**Rate Limiting**\n\n• ClickUp API has rate limits (100 requests per minute per token)\n• Implement request queuing for high-volume workflows\n• Use batch operations when possible\n• Cache frequently accessed data"
        },
        {
          "id": "troubleshooting",
          "title": "6. Troubleshooting Common Issues",
          "content": "**Issue: Webhook not receiving events**\n\nSolution:\n- Verify webhook is registered in ClickUp (Settings → Integrations)\n- Check webhook endpoint URL is accessible\n- Ensure workflow is activated in n8n\n- Test with ClickUp's webhook testing tool\n\n**Issue: Access token expired**\n\nSolution:\n- OAuth tokens can expire\n- Reconnect your ClickUp account in n8n credentials\n- Implement automatic token refresh if using manual tokens\n\n**Issue: API rate limit exceeded**\n\nSolution:\n- Reduce request frequency\n- Implement request queuing\n- Use webhooks instead of polling when possible\n- Consider using multiple OAuth apps for different workflows"
        },
        {
          "id": "complete-example",
          "title": "7. Complete Workflow Example",
          "content": "Here's a complete example workflow that creates a task in ClickUp when a webhook is triggered:\n\n```json\n{\n  \"nodes\": [\n    {\n      \"name\": \"ClickUp Webhook\",\n      \"type\": \"n8n-nodes-base.clickUpTrigger\",\n      \"parameters\": {\n        \"events\": [\"taskCreated\", \"taskUpdated\"],\n        \"workspace\": \"123456\"\n      }\n    },\n    {\n      \"name\": \"Process Data\",\n      \"type\": \"n8n-nodes-base.function\",\n      \"parameters\": {\n        \"functionCode\": \"// Extract task data\\nconst task = items[0].json;\\nreturn [{ json: { taskName: task.name, status: task.status } }];\"\n      }\n    },\n    {\n      \"name\": \"Create ClickUp Task\",\n      \"type\": \"n8n-nodes-base.clickUp\",\n      \"parameters\": {\n        \"operation\": \"create\",\n        \"resource\": \"task\",\n        \"listId\": \"987654\",\n        \"name\": \"={{$json.taskName}}\"\n      }\n    }\n  ]\n}\n```"
        },
        {
          "id": "conclusion",
          "title": "8. Conclusion",
          "content": "Integrating n8n with ClickUp requires careful attention to OAuth authentication and webhook management. Key takeaways:\n\n• Always create unique webhooks for each workflow - never duplicate webhook nodes\n• Properly configure OAuth 2.0 with correct redirect URLs\n• Implement robust error handling and retry logic\n• Monitor webhook registrations and clean up unused ones\n• Follow ClickUp API rate limits and best practices\n\nBy following this guide, you can build reliable ClickUp automation workflows in n8n without encountering webhook ID conflicts or authentication issues."
        }
      ]
    },
    {
      "id": "llm-structured-output-tool-calling",
      "title": "LLM Structured Output: Just Tool Calling in Disguise",
      "slug": "llm-structured-output-tool-calling",
      "category": "research",
      "categoryLabel": "Research",
      "date": "2025-11-13",
      "readTime": "7 min read",
      "author": {
        "name": "Duc Le",
        "avatar": "data/images/image_avatar.jpg",
        "title": "AI Engineer & Researcher"
      },
      "excerpt": "A deep dive into how structured output in modern LLMs is actually implemented through tool calling mechanisms, not as a separate capability. Understanding this reveals important insights about LLM architecture and constraints.",
      "image": "https://via.placeholder.com/1200x600",
      "tags": ["LLM", "AI", "Structured Output", "Tool Calling", "JSON", "OpenAI", "Anthropic"],
      "featured": true,
      "sections": [
        {
          "id": "introduction",
          "title": "1. Introduction",
          "content": "Many developers believe that LLMs have a special \"structured output\" mode that generates perfectly formatted JSON or XML. In reality, most modern LLM APIs implement structured output through their existing tool calling infrastructure—it's essentially a clever wrapper around function calling."
        },
        {
          "id": "what-is-structured-output",
          "title": "2. What is Structured Output?",
          "content": "Structured output refers to the ability of an LLM to generate responses that conform to a specific schema or format, typically JSON. This is useful for:\n\n• Extracting structured data from unstructured text\n• Generating API-ready responses\n• Ensuring type safety in downstream applications\n• Building reliable AI pipelines\n\nExample:\n```json\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"email\": \"john@example.com\"\n}\n```"
        },
        {
          "id": "tool-calling-mechanism",
          "title": "3. The Tool Calling Mechanism",
          "content": "Tool calling (also known as function calling) is a feature where LLMs can decide to invoke external functions with specific parameters. The process works like this:\n\n1. You provide the LLM with function definitions (schemas)\n2. The LLM analyzes the user's request\n3. If appropriate, it generates a function call with properly formatted arguments\n4. Your application executes the function and returns results\n5. The LLM incorporates the results into its response"
        },
        {
          "id": "structured-output-implementation",
          "title": "4. How Structured Output Uses Tool Calling",
          "content": "When you request structured output, here's what actually happens behind the scenes:",
          "nestedList": [
            {
              "text": "Schema Definition",
              "children": [
                "You provide a JSON schema defining your desired output structure",
                "The API converts this schema into a \"virtual tool\" or function definition",
                "This tool has a name like 'generate_structured_output' or similar"
              ]
            },
            {
              "text": "Forced Tool Call",
              "children": [
                "The API instructs the LLM to always call this virtual tool",
                "The LLM treats your schema as function parameters",
                "It generates the structured data as tool arguments"
              ]
            },
            {
              "text": "Response Formatting",
              "children": [
                "The API intercepts the tool call response",
                "It extracts the structured arguments",
                "Returns them directly to you as 'structured output'"
              ]
            }
          ]
        },
        {
          "id": "evidence",
          "title": "5. Evidence from API Providers",
          "content": "Several pieces of evidence support this implementation approach:",
          "table": {
            "headers": ["Provider", "Evidence", "Details"],
            "rows": [
              ["OpenAI", "API Parameter Names", "The 'response_format' parameter and 'tools' parameter share similar schema definitions. Structured output uses 'json_schema' type which mirrors tool definitions."],
              ["Anthropic Claude", "Tool Use Architecture", "Claude's structured output is explicitly implemented through their tool use system. You can see this in their API documentation."],
              ["Streaming Limitation", "No Streaming Support", "Structured output mode does NOT support streaming, just like tool calls must be generated completely before being returned. This is because the entire JSON structure must be validated against the schema."],
              ["Constraint Behavior", "Error Patterns", "Structured output fails in the same ways as tool calling: schema violations, missing required fields, type mismatches."],
              ["Token Limits", "Shared Constraints", "Both structured output and tool calling share the same token limitations for schemas and responses."]
            ]
          }
        },
        {
          "id": "why-this-matters",
          "title": "6. Why This Implementation Matters",
          "content": "Understanding that structured output is tool calling has practical implications:",
          "nestedList": [
            {
              "text": "Performance Characteristics",
              "children": [
                "Structured output has the same latency as tool calling",
                "Token usage follows tool calling patterns",
                "Complexity limits are inherited from tool definitions"
              ]
            },
            {
              "text": "Reliability Considerations",
              "children": [
                "Schema validation happens at the tool level",
                "Complex nested structures may fail like complex tool schemas",
                "Retry logic should mirror tool calling strategies"
              ]
            },
            {
              "text": "Feature Limitations",
              "children": [
                "Can't use structured output AND custom tools simultaneously in some APIs",
                "Schema complexity is bounded by tool definition limits",
                "Some models support tools but not 'structured output' (they're the same!)"
              ]
            }
          ]
        },
        {
          "id": "code-example",
          "title": "7. Practical Example",
          "content": "Here's how you might implement structured output manually using tool calling:\n\n```python\n# Instead of using 'structured output' API\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"extract_user_info\",\n            \"description\": \"Extract user information\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"age\": {\"type\": \"integer\"},\n                    \"email\": {\"type\": \"string\"}\n                },\n                \"required\": [\"name\", \"age\", \"email\"]\n            }\n        }\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"John is 30 and his email is john@example.com\"}],\n    tools=tools,\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_user_info\"}}\n)\n\n# Extract the structured data from tool call\nstructured_data = response.choices[0].message.tool_calls[0].function.arguments\n```\n\nThis is essentially what the structured output API does internally!"
        },
        {
          "id": "best-practices",
          "title": "8. Best Practices",
          "content": "Keep schemas simple and flat when possible\n\nTest with tool calling first to understand failure modes\n\nImplement the same error handling as you would for tools\n\nConsider token costs—complex schemas increase prompt tokens\n\nUse validation libraries to verify output even with structured mode\n\nBe aware that not all models support the same schema complexity"
        },
        {
          "id": "conclusion",
          "title": "9. Conclusion",
          "content": "Structured output in LLMs isn't a magical new capability—it's a well-designed abstraction over tool calling. By understanding this implementation detail, you can:\n\n• Better debug issues with structured output\n• Optimize your schemas for better performance\n• Make informed decisions about when to use structured output vs. manual parsing\n• Understand the limitations and constraints you'll face\n\nThe next time an API provider announces 'improved structured output,' check if they're really just improving their tool calling infrastructure. Chances are, they are!"
        }
      ]
    },
    {
      "id": "api-request-data-transmission",
      "title": "API Request Data Transmission Guide",
      "slug": "api-request-data-transmission",
      "category": "tutorial",
      "categoryLabel": "Tutorial",
      "date": "2025-11-10",
      "readTime": "6 min read",
      "author": {
        "name": "Duc Le",
        "avatar": "data/images/image_avatar.jpg",
        "title": "Software Engineer & Researcher"
      },
      "excerpt": "A comprehensive guide to understanding parameters in GET requests and body payloads in POST requests, with practical examples and best practices for API development.",
      "image": "https://via.placeholder.com/1200x600",
      "tags": ["API", "HTTP", "REST", "GET", "POST", "Web Development"],
      "featured": true,
      "sections": [
        {
          "id": "parameters-get",
          "title": "1. Parameters for GET Requests",
          "content": "Parameters are pieces of information sent along with the URL to filter, sort, paginate, or identify resources in a GET request. They do not modify server data."
        },
        {
          "id": "parameter-types",
          "title": "Types of Parameters",
          "content": "There are three main types of parameters used in GET requests."
        },
        {
          "id": "path-parameters",
          "title": "Path Parameters",
          "content": "Appear in the URL path.\n\nUsed to identify specific resources.\n\nExample:\n```\nGET /users/123\n```\n\nHere `123` identifies a specific user."
        },
        {
          "id": "query-parameters",
          "title": "Query Parameters",
          "content": "Appear after `?` in the URL.\n\nUsed for filtering, sorting, and pagination.\n\nExample:\n```\nGET /users?role=admin&page=2&limit=10\n```"
        },
        {
          "id": "header-parameters",
          "title": "Header Parameters (Optional)",
          "content": "Sent in the HTTP headers.\n\nOften used for authentication or specifying response format.\n\nExample:\n```\nAuthorization: Bearer <token>\nAccept-Language: en-US\n```"
        },
        {
          "id": "get-guidelines",
          "title": "Guidelines for GET Parameters",
          "content": "Use parameters for retrieving or filtering data only.\n\nKeep query parameters short; avoid sending sensitive information.\n\nDo not include large payloads in GET requests; use POST instead."
        },
        {
          "id": "body-post",
          "title": "2. Body for POST Requests",
          "content": "The body of a POST request contains the main data payload sent to the server, often used to create new resources. It may include structured data such as JSON, XML, or form data."
        },
        {
          "id": "body-formats",
          "title": "Body Formats",
          "content": "**application/json** - JSON object\nExample: `{ \"name\": \"John\", \"email\": \"john@example.com\" }`\n\n**application/x-www-form-urlencoded** - Key-value pairs\nExample: `name=John&email=john@example.com`\n\n**multipart/form-data** - Mixed data, often including files\nExample: Form submission with file upload"
        },
        {
          "id": "post-example",
          "title": "POST Request Example",
          "content": "```\nPOST /users\nHost: example.com\nContent-Type: application/json\n\n{\n  \"name\": \"Le Ngoc Duc\",\n  \"email\": \"duc@example.com\",\n  \"password\": \"secure123\"\n}\n```"
        },
        {
          "id": "post-guidelines",
          "title": "Guidelines for POST Body",
          "content": "Use POST body for creating resources or submitting forms.\n\nInclude complex or sensitive data in the body, not in query parameters.\n\nAlways set the appropriate Content-Type header.\n\nKeep the payload structured and validate it server-side."
        },
        {
          "id": "summary",
          "title": "3. Summary",
          "content": "**GET Request** - Data Location: Parameters (Path / Query / Header) - Purpose: Retrieve or filter resources\n\n**POST Request** - Data Location: Body / Payload - Purpose: Create new resources or submit data\n\n💡 Note: In API terminology, payload usually refers to the data inside the body — the actual content sent to the server."
        }
      ]
    },
    {
      "id": "normal-distribution-ai",
      "title": "Use of Normal Distribution in AI: Causes and Effects",
      "slug": "normal-distribution-ai",
      "category": "research",
      "categoryLabel": "Research",
      "date": "2025-11-10",
      "readTime": "10 min read",
      "author": {
        "name": "Duc Le",
        "avatar": "data/images/image_avatar.jpg",
        "title": "Software Engineer & Researcher"
      },
      "excerpt": "A comprehensive technical analysis of why normal distribution is used in AI systems, distinguishing between mathematical causes and practical benefits in modeling, optimization, and generative processes.",
      "image": "https://via.placeholder.com/1200x600",
      "tags": ["AI", "Machine Learning", "Statistics", "Probability", "Deep Learning", "Research"],
      "featured": true,
      "sections": [
        {
          "id": "introduction",
          "title": "1. Introduction",
          "content": "The normal distribution, also known as Gaussian distribution, is one of the most widely used probability distributions in AI. Its prevalence is due to both mathematical properties and practical benefits in modeling, optimization, and generative processes.\n\nThis document explicitly separates causes (why we assume/use normal distribution) from effects (what advantages or outcomes result from this assumption)."
        },
        {
          "id": "causes",
          "title": "2. Causes (Why Normal Distribution is Used)",
          "content": "Understanding why normal distribution is fundamental to AI requires examining several mathematical and practical foundations.",
          "table": {
            "headers": ["Cause", "Explanation"],
            "rows": [
              ["Central Limit Theorem (CLT)", "When many independent random variables are summed, their normalized sum tends toward a Gaussian distribution. Many features, errors, or aggregated signals in AI naturally approximate this."],
              ["Natural Occurrence in Data", "Real-world phenomena, measurement noise, and environmental variations often follow Gaussian-like patterns."],
              ["Mathematical Convenience", "Gaussian distributions have closed-form expressions for probability density, conditional probability, and convolution, which simplifies probabilistic modeling and derivation."],
              ["Smoothness of the Distribution", "Normal distribution is continuous, differentiable, and fully characterized by mean and variance, which facilitates optimization and learning in neural networks."],
              ["Prior Knowledge in Probabilistic Models", "In Bayesian models, assuming Gaussian priors or likelihoods simplifies posterior computation (e.g., conjugate priors)."],
              ["Generative Process Assumptions", "In models like diffusion models or VAEs, Gaussian noise is assumed in the latent or input space to enable sampling and reconstruction."],
              ["Weight Initialization in Neural Networks", "Randomly initializing weights with Gaussian distributions stabilizes forward activations and backward gradients."]
            ]
          }
        },
        {
          "id": "effects",
          "title": "3. Effects / Benefits (What Using Normal Distribution Gives)",
          "content": "The use of normal distribution in AI systems produces several concrete benefits and practical advantages.",
          "table": {
            "headers": ["Effect", "Linked Cause", "Explanation"],
            "rows": [
              ["Efficient Probabilistic Computation", "Mathematical convenience", "Allows closed-form calculations for marginal and conditional probabilities. Useful in Gaussian Naive Bayes, Kalman filters, and Gaussian Processes."],
              ["Stable Training of Neural Networks", "Weight initialization & smoothness", "Gaussian initialization prevents gradient vanishing/explosion, enabling better convergence."],
              ["Controlled Noise Injection", "Generative process assumptions", "Adding Gaussian noise during training or diffusion improves robustness and sample diversity."],
              ["Latent Variable Modeling", "Prior knowledge in probabilistic models", "Enables efficient sampling, reparameterization trick, and approximate inference in VAEs."],
              ["Better Approximation of Aggregate Effects", "CLT", "Justifies modeling errors or combined feature effects as Gaussian, which simplifies model assumptions and optimization."],
              ["Predictable Statistical Properties", "Mathematical convenience", "Mean, variance, and higher moments are well-defined and interpretable, aiding analysis and debugging."]
            ]
          }
        },
        {
          "id": "examples",
          "title": "4. Examples in AI Models",
          "content": "Let's examine how normal distribution is used in specific AI models and techniques.",
          "nestedList": [
            {
              "text": "Kalman Filter",
              "children": [
                "Use: Process and measurement noise assumed Gaussian",
                "Cause: CLT, measurement errors",
                "Effect: Closed-form recursive estimation"
              ]
            },
            {
              "text": "Variational Autoencoder (VAE)",
              "children": [
                "Use: Latent variables follow Gaussian prior",
                "Cause: Probabilistic prior assumption",
                "Effect: Easy sampling and backpropagation via reparameterization trick"
              ]
            },
            {
              "text": "Diffusion Models",
              "children": [
                "Use: Gaussian noise added in forward process",
                "Cause: Generative process assumption",
                "Effect: Enables controlled denoising and reconstruction"
              ]
            },
            {
              "text": "Neural Network Weight Initialization",
              "children": [
                "Use: Random Gaussian initialization",
                "Cause: Smooth, differentiable distribution",
                "Effect: Stable training and faster convergence"
              ]
            },
            {
              "text": "Gaussian Naive Bayes",
              "children": [
                "Use: Features modeled with Gaussian likelihood",
                "Cause: Data approximation",
                "Effect: Tractable classification probabilities"
              ]
            }
          ]
        },
        {
          "id": "summary",
          "title": "5. Summary",
          "content": "Causes are rooted in natural data properties, mathematical convenience, and assumptions made to simplify modeling.\n\nEffects are practical benefits in computation, model stability, generative performance, and probabilistic reasoning.\n\nKey Insight: Understanding this distinction helps AI practitioners choose appropriate models, justify assumptions, and anticipate model behavior."
        }
      ]
    },
    {
      "id": "sse-read-timeout-error",
      "title": "Fixing SSE ReadTimeout Error in MCP Client",
      "slug": "sse-read-timeout-error",
      "category": "tutorial",
      "categoryLabel": "Tutorial",
      "date": "2025-11-10",
      "readTime": "8 min read",
      "author": {
        "name": "Duc Le",
        "avatar": "data/images/image_avatar.jpg",
        "title": "Software Engineer & Researcher"
      },
      "excerpt": "A comprehensive guide to diagnosing and resolving Server-Sent Events (SSE) ReadTimeout errors in MCP client applications, with practical solutions and best practices.",
      "image": "https://via.placeholder.com/1200x600",
      "tags": ["SSE", "Python", "httpx", "Error Handling", "Real-time", "Debugging"],
      "featured": true,
      "sections": [
        {
          "id": "issue-description",
          "title": "I. Issue Description",
          "content": "This error occurs when the client attempts to read data from a Server-Sent Events (SSE) connection, but the server takes too long to respond. The underlying cause is a ReadTimeout raised by httpx, which wraps a lower-level httpcore.ReadTimeout."
        },
        {
          "id": "error-log",
          "title": "Error Log",
          "content": "```\n2025-11-10 02:19:08,976 - mcp.client.sse - ERROR - Error in sse_reader\nTraceback (most recent call last):\n  ...\nhttpx.ReadTimeout\n```"
        },
        {
          "id": "impact",
          "title": "Impact",
          "content": "The client stops receiving SSE messages from the server.\n\nReal-time event streaming is interrupted.\n\nDepending on retry configuration, the process may hang or attempt reconnection repeatedly."
        },
        {
          "id": "root-cause",
          "title": "Root Cause",
          "content": "The server response time exceeds the configured timeout threshold in the HTTP client (httpx). This can happen due to:\n\nHigh latency in the downstream service.\n\nNetwork congestion or unstable connection.\n\nLong-running tasks on the server delaying response streaming."
        },
        {
          "id": "solution",
          "title": "II. Solution",
          "content": "There are three main approaches to resolving this issue, depending on your specific use case and requirements."
        },
        {
          "id": "option1",
          "title": "Option 1 — Increase the HTTP Timeout",
          "content": "Adjust the timeout settings in the HTTP client to allow longer response times.\n\nExample:\n```python\nasync with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:\n    ...\n```\n\nUse this if the server is known to produce delayed but valid responses."
        },
        {
          "id": "option2",
          "title": "Option 2 — Reduce Latency in the Service",
          "content": "Optimize the server-side SSE response logic to send heartbeat events more frequently.\n\nEnsure that long-running operations are processed asynchronously or offloaded to background tasks.\n\nValidate that network bandwidth and load balancer configurations are not throttling streaming responses."
        },
        {
          "id": "option3",
          "title": "Option 3 — Implement Retry and Backoff Logic",
          "content": "Add automatic reconnection logic when ReadTimeout occurs.\n\nUse exponential backoff to prevent flooding the server with reconnection attempts.\n\nExample:\n```python\nimport asyncio\nfrom httpx import ReadTimeout\n\nmax_retries = 3\nbackoff_factor = 2\n\nfor attempt in range(max_retries):\n    try:\n        async with client.stream('GET', sse_url) as response:\n            async for line in response.aiter_lines():\n                process_sse_event(line)\n        break\n    except ReadTimeout:\n        if attempt < max_retries - 1:\n            wait_time = backoff_factor ** attempt\n            await asyncio.sleep(wait_time)\n        else:\n            raise\n```"
        },
        {
          "id": "best-practices",
          "title": "Best Practices",
          "content": "Always configure appropriate timeout values based on your application's requirements.\n\nImplement health check endpoints to monitor SSE connection status.\n\nUse logging to track timeout occurrences and identify patterns.\n\nConsider implementing circuit breaker patterns for production systems.\n\nMonitor server performance metrics to identify bottlenecks early."
        },
        {
          "id": "conclusion",
          "title": "Conclusion",
          "content": "SSE ReadTimeout errors are common in real-time streaming applications. By understanding the root cause and implementing appropriate solutions—whether through timeout adjustments, server optimization, or retry logic—you can build more resilient systems that handle network variability gracefully."
        }
      ]
    }
  ]
}
